<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="Introduction to Finetuning Finetuning is a crucial technique in adapting pre-trained language models for specific tasks or domains. Here&rsquo;s a visual representation of the finetuning process:"><meta property="og:title" content="Understanding LLM finetuning and QLoRA"><meta property="og:description" content="Introduction to Finetuning Finetuning is a crucial technique in adapting pre-trained language models for specific tasks or domains. Here&rsquo;s a visual representation of the finetuning process:"><meta property="og:type" content="website"><meta property="og:image" content="https://secondbrain.chiragsehra.dev/icon.png"><meta property="og:url" content="https://secondbrain.chiragsehra.dev/notes/coding/LLM-Finetuning-and-QLoRA/"><meta property="og:width" content="200"><meta property="og:height" content="200"><meta name=twitter:card content="summary"><meta name=twitter:title content="Understanding LLM finetuning and QLoRA"><meta name=twitter:description content="Introduction to Finetuning Finetuning is a crucial technique in adapting pre-trained language models for specific tasks or domains. Here&rsquo;s a visual representation of the finetuning process:"><meta name=twitter:image content="https://secondbrain.chiragsehra.dev/icon.png"><meta name=twitter:site content="HashChirag"><title>Understanding LLM finetuning and QLoRA</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://secondbrain.chiragsehra.dev//icon.png><link href=https://secondbrain.chiragsehra.dev/styles.19109a40042e9f0e72e952fda4442a34.min.css rel=stylesheet><link href=https://secondbrain.chiragsehra.dev/styles/_light_syntax.86a48a52faebeaaf42158b72922b1c90.min.css rel=stylesheet id=theme-link><script src=https://secondbrain.chiragsehra.dev/js/darkmode.3940b1339c02f0b5a44481e778c4fb21.min.js></script>
<script src=https://secondbrain.chiragsehra.dev/js/util.00639692264b21bc3ee219733d38a8be.min.js></script>
<link rel=preload href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css as=style onload='this.onload=null,this.rel="stylesheet"' integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/copy-tex.min.js integrity=sha384-ww/583aHhxWkz5DEVn6OKtNiIaLi2iBRNZXfJRiY1Ai7tnJ9UXpEsyvOITVpTl4A crossorigin=anonymous></script>
<script type=module>
      import mermaid from 'https://unpkg.com/mermaid@9/dist/mermaid.esm.min.mjs';
      mermaid.initialize({ startOnLoad: true });
    </script><script src=https://cdn.jsdelivr.net/npm/@floating-ui/core@1.2.1></script>
<script src=https://cdn.jsdelivr.net/npm/@floating-ui/dom@1.2.1></script>
<script defer src=https://secondbrain.chiragsehra.dev/js/popover.aa9bc99c7c38d3ae9538f218f1416adb.min.js></script>
<script defer src=https://secondbrain.chiragsehra.dev/js/code-title.ce4a43f09239a9efb48fee342e8ef2df.min.js></script>
<script defer src=https://secondbrain.chiragsehra.dev/js/clipboard.2913da76d3cb21c5deaa4bae7da38c9f.min.js></script>
<script defer src=https://secondbrain.chiragsehra.dev/js/callouts.7723cac461d613d118ee8bb8216b9838.min.js></script>
<script>const SEARCH_ENABLED=!1,LATEX_ENABLED=!0,PRODUCTION=!0,BASE_URL="https://secondbrain.chiragsehra.dev/",fetchData=Promise.all([fetch("https://secondbrain.chiragsehra.dev/indices/linkIndex.1f10ea9197140e793acf659dd6536d9b.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://secondbrain.chiragsehra.dev/indices/contentIndex.f1b21b8d569af9e183d4b53f685d5090.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const e=new URL(BASE_URL),t=e.pathname,n=window.location.pathname,s=t==n;addCopyButtons(),addTitleToCodeBlocks(),addCollapsibleCallouts(),initPopover("https://secondbrain.chiragsehra.dev",!0);const o=document.getElementById("footer");if(o){const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=s&&!1;drawGraph("https://secondbrain.chiragsehra.dev",t,[{"/moc":"#4388cc"}],t?{centerForce:1,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.5,linkDistance:1,opacityScale:3,repelForce:1,scale:1.4}:{centerForce:1,depth:1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:1,opacityScale:3,repelForce:2,scale:1.2})}var i=document.getElementsByClassName("mermaid");i.length>0&&import("https://unpkg.com/mermaid@9/dist/mermaid.esm.min.mjs").then(e=>{e.default.init()});function a(n){const e=n.target,t=e.className.split(" "),s=t.includes("broken"),o=t.includes("internal-link");plausible("Link Click",{props:{href:e.href,broken:s,internal:o,graph:!1}})}const r=document.querySelectorAll("a");for(link of r)link.className.includes("root-title")&&link.addEventListener("click",a,{once:!0})},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],macros:{'â€™':"'"},throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/secondbrain.chiragsehra.dev\/js\/router.d6fe6bd821db9ea97f9aeefae814d8e7.min.js"
    attachSPARouting(init, render)
  </script><script defer data-domain=secondbrain.chiragsehra.dev src=https://plausible.io/js/script.js></script>
<script>window.plausible=window.plausible||function(){(window.plausible.q=window.plausible.q||[]).push(arguments)}</script></head><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://secondbrain.chiragsehra.dev/js/full-text-search.e6e2e0c213187ca0c703d6e2c7a77fcd.min.js></script><div class=singlePage><header><h1 id=page-title><a class=root-title href=https://secondbrain.chiragsehra.dev/>ðŸ§  Sehra's Second Brain</a></h1><div class=spacer></div><div id=search-icon><p>Search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>Understanding LLM finetuning and QLoRA</h1><p class=meta>Last updated
Dec 1, 2024
<a href=https://github.com/chiragsehra/quartz/tree/hugo/content/notes/coding/LLM%20Finetuning%20and%20QLoRA.md rel=noopener>Edit Source</a></p><ul class=tags></ul><aside class=mainTOC><details><summary>Table of Contents</summary><nav id=TableOfContents><ol><li><a href=#introduction-to-finetuning>Introduction to Finetuning</a><ol><li><a href=#key-concepts>Key Concepts</a></li></ol></li><li><a href=#memory-challenges-in-finetuning>Memory Challenges in Finetuning</a></li><li><a href=#quantisation-overview>Quantisation Overview</a></li><li><a href=#advanced-quantisation-techniques>Advanced Quantisation Techniques</a><ol><li><a href=#1-4-bit-normalfloat>1. 4-bit NormalFloat</a></li><li><a href=#2-double-quantisation>2. Double Quantisation</a></li><li><a href=#3-paged-optimiser>3. Paged Optimiser</a></li><li><a href=#4-lora-low-rank-adaptation>4. LoRA (Low-Rank Adaptation)</a></li></ol></li><li><a href=#comparison-of-finetuning-approaches>Comparison of Finetuning Approaches</a><ol><li><a href=#detailed-comparison>Detailed Comparison</a></li></ol></li><li><a href=#practical-implications>Practical Implications</a></li><li><a href=#conclusion>Conclusion</a></li></ol></nav></details></aside><hr><a href=#introduction-to-finetuning><h2 id=introduction-to-finetuning><span class=hanchor arialabel=Anchor># </span>Introduction to Finetuning</h2></a><p>Finetuning is a crucial technique in adapting pre-trained language models for specific tasks or domains. Here&rsquo;s a visual representation of the finetuning process:</p><div class=mermaid>graph TD
A[Pre-trained LLM] --> B[General Knowledge]
A --> C[Generic Capabilities]
B --> D[Finetuning]
C --> D
D --> E[Task-specific Model]
D --> F[Domain-specific Knowledge]
subgraph Memory Requirements
G[Full Model Parameters]
H[GPU Memory]
I[Training Data]
end
G --> J[Memory Challenge]
H --> J
I --> J
style A fill:#f9f,stroke:#333
style D fill:#bbf,stroke:#333
style J fill:#f99,stroke:#333</div><a href=#key-concepts><h3 id=key-concepts><span class=hanchor arialabel=Anchor># </span>Key Concepts</h3></a><ul><li>Pre-trained models come with general knowledge and capabilities</li><li>Finetuning adapts these capabilities to specific use cases</li><li>The process requires significant computational resources</li></ul><hr><a href=#memory-challenges-in-finetuning><h2 id=memory-challenges-in-finetuning><span class=hanchor arialabel=Anchor># </span>Memory Challenges in Finetuning</h2></a><p>The primary challenge in finetuning Large Language Models lies in their enormous memory requirements. Let&rsquo;s break down the memory needs for a typical 13B parameter model:</p><ol><li><p><strong>Base Model Storage</strong></p><ul><li>Each parameter: 32-bit floating-point (4 bytes)</li><li>Total size: 13B Ã— 4 bytes = 52GB</li></ul></li><li><p><strong>Training Requirements</strong></p><ul><li>Model parameters: 52GB</li><li>Gradients: 52GB</li><li>Optimiser states: 104GB (2 copies)</li><li>Total: ~208GB GPU memory</li></ul></li></ol><hr><a href=#quantisation-overview><h2 id=quantisation-overview><span class=hanchor arialabel=Anchor># </span>Quantisation Overview</h2></a><p>Quantisation offers a solution to memory challenges. Here&rsquo;s a visualisation of quantisation and memory optimisation techniques:</p><div class=mermaid>graph TD
A[Original Model Parameters] --> B[32-bit Floating Point]
B --> C[Quantization]
C --> D[4-bit NormalFloat]
C --> E[Double Quantization]
D --> F[Reduced Memory Usage]
E --> F
subgraph Memory Optimization
G[Paged Optimizer]
H[CPU Memory Offloading]
I[GPU Memory Management]
end
F --> G
G --> H
G --> I
style C fill:#bbf,stroke:#333
style F fill:#9f9,stroke:#333
style G fill:#ff9,stroke:#333</div><hr><a href=#advanced-quantisation-techniques><h2 id=advanced-quantisation-techniques><span class=hanchor arialabel=Anchor># </span>Advanced Quantisation Techniques</h2></a><a href=#1-4-bit-normalfloat><h3 id=1-4-bit-normalfloat><span class=hanchor arialabel=Anchor># </span>1. 4-bit NormalFloat</h3></a><ul><li>Reduces precision from 32-bit to 4-bit</li><li>Preserves normal distribution of weights</li><li>Memory reduction: 87.5%</li><li>Example: -0.765432 (32-bit) â†’ -0.75 (4-bit)</li></ul><hr><a href=#2-double-quantisation><h3 id=2-double-quantisation><span class=hanchor arialabel=Anchor># </span>2. Double Quantisation</h3></a><ul><li>Two-step quantisation process:<ol><li>Quantise weights to 4-bit</li><li>Quantise scaling factors</li></ol></li><li>Additional 10-20% memory reduction</li></ul><hr><a href=#3-paged-optimiser><h3 id=3-paged-optimiser><span class=hanchor arialabel=Anchor># </span>3. Paged Optimiser</h3></a><ul><li>CPU memory utilisation for parameter storage</li><li>Step-by-step process:<ol><li>Load parameter batch to GPU</li><li>Compute gradients</li><li>Update parameters</li><li>Store in CPU memory</li><li>Repeat with next batch</li></ol></li></ul><hr><a href=#4-lora-low-rank-adaptation><h3 id=4-lora-low-rank-adaptation><span class=hanchor arialabel=Anchor># </span>4. LoRA (Low-Rank Adaptation)</h3></a><ul><li>Matrix decomposition approach</li><li>Example:<ul><li>Original: 1000Ã—1000 matrix</li><li>Decomposed: 1000Ã—8 and 8Ã—1000 matrices</li><li>Parameter reduction: >99%</li></ul></li></ul><hr><a href=#comparison-of-finetuning-approaches><h2 id=comparison-of-finetuning-approaches><span class=hanchor arialabel=Anchor># </span>Comparison of Finetuning Approaches</h2></a><p>Here&rsquo;s a visual comparison of different finetuning methods:</p><div class=mermaid>graph TD
A[Finetuning Approaches]
A --> B[Full Finetuning]
A --> C[LoRA]
A --> D[QLoRA]
B --> E[Updates all parameters<br>High memory usage<br>Best performance]
C --> F[Low-rank adaptation<br>Few trainable parameters<br>Moderate performance]
D --> G[4-bit quantization + LoRA<br>Minimal memory usage<br>Near SOTA performance]
subgraph Memory Requirements
H[200+ GB]
I[20-40 GB]
J[10-20 GB]
end
E --> H
F --> I
G --> J
style A fill:#f9f,stroke:#333
style B fill:#bbf,stroke:#333
style C fill:#bbf,stroke:#333
style D fill:#bbf,stroke:#333</div><hr><a href=#detailed-comparison><h3 id=detailed-comparison><span class=hanchor arialabel=Anchor># </span>Detailed Comparison</h3></a><ol><li><p><strong>Full Finetuning</strong></p><ul><li>Updates all model parameters</li><li>Memory requirement: 200GB+ (13B model)</li><li>Best possible performance</li><li>Limited by GPU availability</li></ul></li><li><p><strong>LoRA</strong></p><ul><li>Trains only rank decomposition matrices</li><li>Memory requirement: 20-40GB</li><li>Good performance</li><li>More accessible hardware requirements</li></ul></li><li><p><strong>QLoRA</strong></p><ul><li>Combines 4-bit quantisation with LoRA</li><li>Memory requirement: 10-20GB</li><li>Near-SOTA performance</li><li>Can run on single consumer GPU</li></ul></li></ol><hr><a href=#practical-implications><h2 id=practical-implications><span class=hanchor arialabel=Anchor># </span>Practical Implications</h2></a><ol><li><p><strong>Hardware Requirements</strong></p><ul><li>Full Finetuning: Multiple high-end GPUs</li><li>LoRA: Single high-end GPU</li><li>QLoRA: Consumer-grade GPU (e.g., RTX 4090)</li></ul></li><li><p><strong>Training Time</strong></p><ul><li>Full Finetuning: Longest training time</li><li>LoRA: Moderate training time</li><li>QLoRA: Similar to LoRA</li></ul></li><li><p><strong>Performance vs Resource Trade-off</strong></p><ul><li>Full Finetuning: Highest performance, highest cost</li><li>LoRA: Good performance, moderate cost</li><li>QLoRA: Near-optimal performance, lowest cost</li></ul></li></ol><hr><a href=#conclusion><h2 id=conclusion><span class=hanchor arialabel=Anchor># </span>Conclusion</h2></a><p>QLoRA represents a significant advancement in making LLM finetuning more accessible while maintaining high performance. By combining quantisation techniques with LoRA, it enables training of large models on consumer hardware, democratising access to LLM customisation.</p></article><hr><div class=page-end id=footer><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li><a href=/notes/coding/coding/ data-ctx="LLM Finetuning and QLoRA" data-src=/notes/coding/coding class=internal-link>coding</a></li></ul></div><div><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://secondbrain.chiragsehra.dev/js/graph.6579af7b10c818dbd2ca038702db0224.js></script></div></div><div id=contact_buttons><footer><p></p><ul><li><a href=https://secondbrain.chiragsehra.dev/>Home</a></li><li><a href=https://twitter.com/HashChirag>Twitter</a></li><li><a href=https://github.com/chiragsehra>GitHub</a></li><li><a href=https://www.linkedin.com/in/chiragsehra>Linkedin</a></li></ul></footer></div></div></body></html>